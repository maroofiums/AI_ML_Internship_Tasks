{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# BERT News Classification ‚Äì Starter Notebook\n\nThis notebook demonstrates an **end-to-end BERT-based text classification pipeline**\nusing the **AG News dataset**.\n\n### Key Highlights\n- Pre-trained **BERT (bert-base-uncased)**\n- Fine-tuned for **multi-class classification**\n- Achieved **~94% accuracy**\n- Deployed using **Gradio**\n","metadata":{}},{"cell_type":"code","source":"# Imports\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom sklearn.metrics import accuracy_score, f1_score\nimport gradio as gr","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T15:32:24.140348Z","iopub.execute_input":"2026-01-31T15:32:24.140686Z","iopub.status.idle":"2026-01-31T15:33:02.927483Z","shell.execute_reply.started":"2026-01-31T15:32:24.140655Z","shell.execute_reply":"2026-01-31T15:33:02.926854Z"}},"outputs":[{"name":"stderr","text":"2026-01-31 15:32:42.261158: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769873562.512686      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769873562.576555      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769873563.165311      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769873563.165353      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769873563.165355      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769873563.165358      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## üìä Dataset Overview\n\nWe use the **AG News Dataset**, which contains news articles categorized into:\n\n- üåç World\n- üèÖ Sports\n- üíº Business\n- üî¨ Science & Technology\n\nEach sample includes:\n- News **title**\n- News **description**\n- Corresponding **label**\n","metadata":{}},{"cell_type":"code","source":"# CSV URLs\ntrain_url = \"https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\"\ntest_url  = \"https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T15:33:02.929169Z","iopub.execute_input":"2026-01-31T15:33:02.929407Z","iopub.status.idle":"2026-01-31T15:33:02.933248Z","shell.execute_reply.started":"2026-01-31T15:33:02.929386Z","shell.execute_reply":"2026-01-31T15:33:02.932676Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Load CSV\ntrain_df = pd.read_csv(train_url, header=None)\ntest_df  = pd.read_csv(test_url, header=None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T15:33:02.934508Z","iopub.execute_input":"2026-01-31T15:33:02.935066Z","iopub.status.idle":"2026-01-31T15:33:05.012652Z","shell.execute_reply.started":"2026-01-31T15:33:02.935027Z","shell.execute_reply":"2026-01-31T15:33:05.011803Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"train_df = train_df.iloc[:, :3]\ntest_df  = test_df.iloc[:, :3]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T15:33:05.013678Z","iopub.execute_input":"2026-01-31T15:33:05.014036Z","iopub.status.idle":"2026-01-31T15:33:05.030507Z","shell.execute_reply.started":"2026-01-31T15:33:05.014003Z","shell.execute_reply":"2026-01-31T15:33:05.029885Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_df.columns = [\"label\", \"title\", \"description\"]\ntest_df.columns  = [\"label\", \"title\", \"description\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T15:33:05.031339Z","iopub.execute_input":"2026-01-31T15:33:05.031625Z","iopub.status.idle":"2026-01-31T15:33:05.035965Z","shell.execute_reply.started":"2026-01-31T15:33:05.031587Z","shell.execute_reply":"2026-01-31T15:33:05.035178Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train_df[\"text\"] = train_df[\"title\"] + \" \" + train_df[\"description\"]\ntest_df[\"text\"]  = test_df[\"title\"] + \" \" + test_df[\"description\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T15:33:05.036981Z","iopub.execute_input":"2026-01-31T15:33:05.037213Z","iopub.status.idle":"2026-01-31T15:33:05.099698Z","shell.execute_reply.started":"2026-01-31T15:33:05.037193Z","shell.execute_reply":"2026-01-31T15:33:05.099148Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_df[\"label\"] = train_df[\"label\"] - 1\ntest_df[\"label\"]  = test_df[\"label\"] - 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T15:33:05.101943Z","iopub.execute_input":"2026-01-31T15:33:05.102148Z","iopub.status.idle":"2026-01-31T15:33:05.106918Z","shell.execute_reply.started":"2026-01-31T15:33:05.102128Z","shell.execute_reply":"2026-01-31T15:33:05.106284Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"train_texts, val_texts, train_labels, val_labels = train_test_split(\n    train_df[\"text\"].tolist(),\n    train_df[\"label\"].tolist(),\n    test_size=0.1,\n    random_state=42\n)\n\ntest_texts  = test_df[\"text\"].tolist()\ntest_labels = test_df[\"label\"].tolist()\n\nprint(f\"Train: {len(train_texts)}, Validation: {len(val_texts)}, Test: {len(test_texts)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T15:33:05.107812Z","iopub.execute_input":"2026-01-31T15:33:05.108063Z","iopub.status.idle":"2026-01-31T15:33:05.166691Z","shell.execute_reply.started":"2026-01-31T15:33:05.108034Z","shell.execute_reply":"2026-01-31T15:33:05.165994Z"}},"outputs":[{"name":"stdout","text":"Train: 108000, Validation: 12000, Test: 7600\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## üßπ Text Preprocessing\n\n- Combined **title + description**\n- Converted labels to start from **0**\n- Split data into:\n  - Training set\n  - Validation set\n  - Test set\n\nBERT tokenizer handles:\n- Tokenization\n- Padding\n- Truncation\n","metadata":{}},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\nclass NewsDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=64):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        encoding = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_len,\n            return_tensors=\"pt\"\n        )\n        return {\n            \"input_ids\": encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n            \"label\": torch.tensor(label, dtype=torch.long)\n        }\n\ntrain_dataset = NewsDataset(train_texts, train_labels, tokenizer)\nval_dataset   = NewsDataset(val_texts, val_labels, tokenizer)\ntest_dataset  = NewsDataset(test_texts, test_labels, tokenizer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T15:33:05.167626Z","iopub.execute_input":"2026-01-31T15:33:05.168009Z","iopub.status.idle":"2026-01-31T15:33:06.794408Z","shell.execute_reply.started":"2026-01-31T15:33:05.167976Z","shell.execute_reply":"2026-01-31T15:33:06.793745Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4836170566504bad98674b768c1eca13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2126b5ca3a7c4b188045588f909b5013"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c029081ccf4346428492b3652c25005f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25d3de52cf8c4a31ab9dd0b0ef3518fc"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader   = DataLoader(val_dataset, batch_size=16)\ntest_loader  = DataLoader(test_dataset, batch_size=16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T15:33:06.795251Z","iopub.execute_input":"2026-01-31T15:33:06.795532Z","iopub.status.idle":"2026-01-31T15:33:06.800061Z","shell.execute_reply.started":"2026-01-31T15:33:06.795496Z","shell.execute_reply":"2026-01-31T15:33:06.799290Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## üß† Custom PyTorch Dataset\n\nWe define a custom `Dataset` class to:\n- Tokenize text on-the-fly\n- Return `input_ids`, `attention_mask`, and `labels`\n- Support batching via `DataLoader`\n\nThis keeps the pipeline clean and scalable.","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=4\n)\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T15:33:06.800950Z","iopub.execute_input":"2026-01-31T15:33:06.801252Z","iopub.status.idle":"2026-01-31T15:33:09.579765Z","shell.execute_reply.started":"2026-01-31T15:33:06.801193Z","shell.execute_reply":"2026-01-31T15:33:09.579139Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"431b9355b6c746848c6ee0cc0106e31b"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=4, bias=True)\n)"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"## ü§ñ Model Architecture\n\nWe use:\n\n- **BERT Base (Uncased)**\n- Hidden size: 768\n- Transformer layers: 12\n- Output layer customized for **4 classes**\n\nThe classification head is trained from scratch.\n","metadata":{}},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=5e-5)\nepochs = 2\n\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for batch in train_loader:\n        optimizer.zero_grad()\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"label\"].to(device)\n\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n        loss = outputs.loss\n        total_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n    \n    print(f\"Epoch {epoch+1} | Avg Loss: {total_loss/len(train_loader):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T15:33:09.580678Z","iopub.execute_input":"2026-01-31T15:33:09.580925Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üöÄ Training Strategy\n\n- Optimizer: **AdamW**\n- Learning Rate: `5e-5`\n- Batch Size: `16`\n- Epochs: `2`\n- Loss Function: **CrossEntropyLoss** (built-in)\n\nTraining performed on **GPU (CUDA)** when available.","metadata":{}},{"cell_type":"code","source":"model.eval()\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"label\"].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n        preds = torch.argmax(outputs.logits, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\nacc = accuracy_score(all_labels, all_preds)\nf1  = f1_score(all_labels, all_preds, average=\"weighted\")\n\nprint(f\"Test Accuracy: {acc:.4f}\")\nprint(f\"Test F1-score: {f1:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìà Model Evaluation\n\nWe evaluate the model using:\n\n- **Accuracy**\n- **Weighted F1-score**\n\nThese metrics ensure balanced performance across all classes.","metadata":{}},{"cell_type":"code","source":"labels = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n\ndef predict_headline(text):\n    encoding = tokenizer(\n        text,\n        padding=\"max_length\",\n        truncation=True,\n        max_length=64,\n        return_tensors=\"pt\"\n    ).to(device)\n    model.eval()\n    with torch.no_grad():\n        logits = model(**encoding).logits\n    pred = torch.argmax(logits, dim=1).item()\n    return labels[pred]\n\ninterface = gr.Interface(\n    fn=predict_headline,\n    inputs=gr.Textbox(lines=2, placeholder=\"Enter a news headline...\"),\n    outputs=\"text\",\n    title=\"News Topic Classifier\",\n    description=\"Enter a news headline and BERT predicts the topic!\"\n)\n\ninterface.launch()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üíæ Saving Model Artifacts\n\nAfter training and evaluation, we save the **fine-tuned BERT model** and its\nassociated components for future use.\n\n### What is saved?\n- ‚úÖ Trained BERT model weights\n- ‚úÖ Tokenizer configuration and vocabulary\n- ‚úÖ Label mapping for class interpretation\n\n### Why is this important?\n- Enables **reuse without retraining**\n- Required for **deployment** (Streamlit / Gradio / FastAPI)\n- Ensures **consistent predictions** across environments\n\nAll artifacts are stored locally and can be easily loaded for inference or production deployment.\n","metadata":{}},{"cell_type":"code","source":"model.save_pretrained(\"bert_news_model\")\ntokenizer.save_pretrained(\"bert_news_model\")\n\nprint(\"Model and tokenizer saved successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_mapping = {\n    0: \"World\",\n    1: \"Sports\",\n    2: \"Business\",\n    3: \"Sci/Tech\"\n}\n\nimport json\nwith open(\"label_mapping.json\", \"w\") as f:\n    json.dump(label_mapping, f)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}